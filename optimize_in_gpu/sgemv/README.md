# SGEMV çŸ©é˜µå‘é‡ä¹˜æ³•ä¼˜åŒ–

å•ç²¾åº¦çŸ©é˜µå‘é‡ä¹˜æ³•ï¼ˆSGEMVï¼‰è®¡ç®— `y = A * x`ï¼Œå…¶ä¸­ A æ˜¯ MÃ—N çŸ©é˜µï¼Œx æ˜¯ N ç»´å‘é‡ï¼Œy æ˜¯ M ç»´å‘é‡ã€‚æœ¬ç›®å½•å±•ç¤ºäº†é’ˆå¯¹ä¸åŒæ•°æ®å½¢çŠ¶çš„ä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

åœ¨ **NVIDIA V100** GPU ä¸Šæµ‹è¯•ï¼š

| ç‰ˆæœ¬ | M | N | æˆ‘çš„å®ç° (ns) | cuBLAS (ns) | æ€§èƒ½æ¯” |
|------|---|---|--------------|-------------|--------|
| v0 | 16384 | 32 | 10341 | 8386 | 81.1% |
| v1 | 16384 | 128 | 14284 | 15848 | **110.9%** |
| v2 | 16384 | 16 | 6903 | 7576 | **109.7%** |

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `Sgemv_v0.cu` - åŸºç¡€ç‰ˆæœ¬ï¼Œé’ˆå¯¹ n=32 çš„æƒ…å†µ
- `Sgemv_v1.cu` - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé’ˆå¯¹ n>32 çš„æƒ…å†µ
- `Sgemv_v2.cu` - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé’ˆå¯¹ n<32 çš„æƒ…å†µ
- `ComplexHalfGemv.cu` - å¤æ•°åŠç²¾åº¦ç‰ˆæœ¬
- `cuHalfComplex.cuh` - å¤æ•°åŠç²¾åº¦å·¥å…·å¤´æ–‡ä»¶

## ğŸ¯ æ ¸å¿ƒä¼˜åŒ–æ€æƒ³

SGEMV ä¼˜åŒ–çš„æ ¸å¿ƒåœ¨äº**åˆç†è®¾è®¡ block å’Œ thread çš„é…ç½®**ï¼Œ**é¿å…çº¿ç¨‹ç©ºé—²**ã€‚

### é—®é¢˜åˆ†æ

SGEMV çš„è®¡ç®—æ¨¡å¼ï¼š
- æ¯ä¸ªè¾“å‡ºå…ƒç´  `y[i]` éœ€è¦è®¡ç®— `A[i, :] * x` çš„ç‚¹ç§¯
- è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ reduce æ“ä½œ
- å…³é”®æ˜¯å¦‚ä½•ç»„ç»‡çº¿ç¨‹æ¥é«˜æ•ˆåœ°å®Œæˆè¿™ä¸ª reduce

### ä¼˜åŒ–ç­–ç•¥

æ ¹æ®å‘é‡ x çš„é•¿åº¦ï¼ˆNï¼‰ä¸åŒï¼Œé‡‡ç”¨ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥ï¼š

#### 1. N = 32 (v0)

- æ¯ä¸ª warpï¼ˆ32 ä¸ªçº¿ç¨‹ï¼‰å¤„ç†ä¸€è¡Œ
- æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ 
- ä½¿ç”¨ warp shuffle è¿›è¡Œ reduce

#### 2. N > 32 (v1)

- æ¯ä¸ª block å¤„ç†å¤šè¡Œ
- ä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨ä¸­é—´ç»“æœ
- å¤šä¸ª warp åä½œå®Œæˆ reduce

#### 3. N < 32 (v2)

- æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šè¡Œ
- å¢åŠ æ¯ä¸ªçº¿ç¨‹çš„å·¥ä½œé‡
- å‡å°‘ block æ•°é‡ï¼Œæé«˜å ç”¨ç‡

## ğŸ”§ å®ç°ç»†èŠ‚

### v0: N = 32 çš„æƒ…å†µ

```cpp
// æ¯ä¸ª warp å¤„ç†ä¸€è¡Œ
// æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ 
// ä½¿ç”¨ warp shuffle è¿›è¡Œ reduce
__global__ void Sgemv_v0(float *A, float *x, float *y, int M, int N) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < M) {
        float sum = 0.0f;
        // æ¯ä¸ªçº¿ç¨‹è®¡ç®—ä¸€ä¸ªå…ƒç´ 
        sum = A[row * N + threadIdx.y] * x[threadIdx.y];
        // ä½¿ç”¨ warp shuffle è¿›è¡Œ reduce
        sum = warpReduceSum<32>(sum);
        if (threadIdx.y == 0) {
            y[row] = sum;
        }
    }
}
```

**ç‰¹ç‚¹ï¼š**
- ç®€å•ç›´æ¥
- é€‚åˆ N æ­£å¥½ç­‰äº warp size çš„æƒ…å†µ
- æ€§èƒ½ï¼š81.1% of cuBLAS

### v1: N > 32 çš„æƒ…å†µ

**ä¼˜åŒ–ç‚¹ï¼š**
- ä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨ä¸­é—´ç»“æœ
- å¤šä¸ª warp åä½œå®Œæˆ reduce
- æ›´å¥½çš„è´Ÿè½½å‡è¡¡

**æ€§èƒ½ï¼š110.9% of cuBLAS** âœ…

### v2: N < 32 çš„æƒ…å†µ

**ä¼˜åŒ–ç‚¹ï¼š**
- æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šè¡Œ
- å¢åŠ æ¯ä¸ªçº¿ç¨‹çš„å·¥ä½œé‡
- å‡å°‘çº¿ç¨‹ç©ºé—²

**æ€§èƒ½ï¼š109.7% of cuBLAS** âœ…

## ğŸ’¡ å…³é”®ä¼˜åŒ–æŠ€å·§

### 1. Warp Shuffle æŒ‡ä»¤

```cpp
template <unsigned int WarpSize>
__device__ __forceinline__ float warpReduceSum(float sum) {
    if (WarpSize >= 32) sum += __shfl_down_sync(0xffffffff, sum, 16);
    if (WarpSize >= 16) sum += __shfl_down_sync(0xffffffff, sum, 8);
    if (WarpSize >= 8) sum += __shfl_down_sync(0xffffffff, sum, 4);
    if (WarpSize >= 4) sum += __shfl_down_sync(0xffffffff, sum, 2);
    if (WarpSize >= 2) sum += __shfl_down_sync(0xffffffff, sum, 1);
    return sum;
}
```

**ä¼˜åŠ¿ï¼š**
- ä¸éœ€è¦å…±äº«å†…å­˜
- å»¶è¿Ÿæ›´ä½
- å¸¦å®½æ›´é«˜ï¼ˆå¯„å­˜å™¨è®¿é—®æ¯”å…±äº«å†…å­˜å¿«ï¼‰

### 2. å‘é‡åŒ–åŠ è½½

å¯¹äºè¾ƒå¤§çš„ Nï¼Œå¯ä»¥ä½¿ç”¨ `float4` å‘é‡åŒ–åŠ è½½ï¼š

```cpp
float4 vec_a = FETCH_FLOAT4(A[row * N + col]);
float4 vec_x = FETCH_FLOAT4(x[col]);
```

### 3. å…±äº«å†…å­˜ä½¿ç”¨

å¯¹äº N > 32 çš„æƒ…å†µï¼Œä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨ä¸­é—´ç»“æœï¼š

```cpp
__shared__ float sdata[BLOCK_SIZE];
// æ¯ä¸ª warp å°†ç»“æœå†™å…¥å…±äº«å†…å­˜
if (lane_id == 0) {
    sdata[warp_id] = sum;
}
__syncthreads();
// ç¬¬ä¸€ä¸ª warp è¿›è¡Œæœ€ç»ˆçš„ reduce
```

## ğŸ“ˆ æ€§èƒ½åˆ†æ

### ä½¿ç”¨ Nsight Systems åˆ†æ

```bash
# ç¼–è¯‘
nvcc -o sgemv_v0 Sgemv_v0.cu -lcublas

# æ€§èƒ½åˆ†æ
nsys profile --trace=cuda,nvtx --output=sgemv_profile.nsys-rep ./sgemv_v0

# æŸ¥çœ‹ç»“æœ
nsys-ui sgemv_profile.nsys-rep
```

### å…³é”®æŒ‡æ ‡

- **å†…å­˜å¸¦å®½åˆ©ç”¨ç‡**: æ£€æŸ¥å…¨å±€å†…å­˜è®¿é—®æ•ˆç‡
- **å ç”¨ç‡**: SM å ç”¨ç‡
- **Warp æ•ˆç‡**: Warp å†…çº¿ç¨‹çš„åˆ©ç”¨ç‡
- **å…±äº«å†…å­˜ä½¿ç”¨**: Bank conflict æƒ…å†µ

## ğŸ“ å­¦ä¹ è¦ç‚¹

1. **ç†è§£æ•°æ®å½¢çŠ¶å¯¹æ€§èƒ½çš„å½±å“**
   - ä¸åŒçš„ N å€¼éœ€è¦ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
   - æ²¡æœ‰ä¸€ç§é€šç”¨çš„ä¼˜åŒ–æ–¹æ³•é€‚ç”¨äºæ‰€æœ‰æƒ…å†µ

2. **åˆç†è®¾è®¡ block å’Œ thread**
   - é¿å…çº¿ç¨‹ç©ºé—²
   - å¹³è¡¡å ç”¨ç‡å’Œèµ„æºä½¿ç”¨

3. **çµæ´»ä½¿ç”¨ warp shuffle**
   - å¯¹äºå°è§„æ¨¡çš„ reduceï¼Œwarp shuffle æ¯”å…±äº«å†…å­˜æ›´é«˜æ•ˆ
   - å‡å°‘å…±äº«å†…å­˜ä½¿ç”¨ï¼Œæé«˜å ç”¨ç‡

4. **é’ˆå¯¹ç‰¹å®šåœºæ™¯ä¼˜åŒ–**
   - æ ¹æ®å®é™…åº”ç”¨åœºæ™¯é€‰æ‹©æœ€åˆé€‚çš„ç‰ˆæœ¬
   - æœ‰æ—¶å¯ä»¥é’ˆå¯¹ç‰¹å®šæ•°æ®å½¢çŠ¶è¿›è¡Œç‰¹æ®Šä¼˜åŒ–

## ğŸ”— ä¸å…¶ä»–ç®—å­çš„å…³ç³»

- **Reduce**: SGEMV æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª per-row çš„ reduce æ“ä½œ
- **SGEMM**: å¯ä»¥ç†è§£ä¸ºå¤šä¸ª SGEMV çš„ç»„åˆ
- **Elementwise**: æŸäº›ä¼˜åŒ–æŠ€å·§ï¼ˆå¦‚å‘é‡åŒ–ï¼‰å¯ä»¥å€Ÿé‰´

## ğŸ“š ç›¸å…³èµ„æº

- [NVIDIA cuBLAS SGEMV](https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemv)
- [CUDA Warp Shuffle](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions)
- [Matrix-Vector Multiplication Optimization](https://developer.nvidia.com/blog/parallelforall/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/)

## ğŸ’» ç¼–è¯‘å’Œè¿è¡Œ

```bash
# ç¼–è¯‘æ‰€æœ‰ç‰ˆæœ¬
nvcc -o sgemv_v0 Sgemv_v0.cu -lcublas
nvcc -o sgemv_v1 Sgemv_v1.cu -lcublas
nvcc -o sgemv_v2 Sgemv_v2.cu -lcublas

# è¿è¡Œ
./sgemv_v0
./sgemv_v1
./sgemv_v2
```

---

**é€šè¿‡é’ˆå¯¹ä¸åŒæ•°æ®å½¢çŠ¶çš„ä¼˜åŒ–ï¼Œå®ç°è¶…è¶Š cuBLAS çš„æ€§èƒ½ï¼** ğŸš€
