# CUDA æœ€ä½³å®è·µå­¦ä¹ æŒ‡å—

æœ¬ç›®å½•åŒ…å«ç³»ç»Ÿæ€§çš„ CUDA ç¼–ç¨‹å­¦ä¹ è·¯å¾„ï¼Œä»åŸºç¡€ç®—å­ä¼˜åŒ–åˆ°é«˜çº§ Tensor Core ä½¿ç”¨ï¼Œå¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£ GPU ç¼–ç¨‹å’Œä¼˜åŒ–æŠ€å·§ã€‚

## ğŸ“š ç›®å½•ç»“æ„

```
universe_best_cuda_practice/
â”œâ”€â”€ 1_cuda_reduce_study/          # Reduce ç®—å­ä¼˜åŒ–ç ”ç©¶ï¼ˆ10ä¸ªç‰ˆæœ¬ï¼‰
â”œâ”€â”€ 2_cuda_sgemm_study/           # SGEMM çŸ©é˜µä¹˜æ³•ä¼˜åŒ–ï¼ˆ8ä¸ªç‰ˆæœ¬ï¼‰
â”œâ”€â”€ 3_kernel_profiling_guide/      # Kernel æ€§èƒ½åˆ†æå’Œä¼˜åŒ–
â”œâ”€â”€ 4_tensor_core_wmma/           # Tensor Core WMMA API ä½¿ç”¨
â”œâ”€â”€ 5_mma_and_swizzle/            # MMA æŒ‡ä»¤å’Œå†…å­˜ Swizzle ä¼˜åŒ–
â”œâ”€â”€ 6_cutlass_study/              # CUTLASS é«˜æ€§èƒ½åº“å­¦ä¹ 
â””â”€â”€ flash_attention/              # Flash Attention å®ç°
```

## ğŸ¯ å­¦ä¹ è·¯å¾„

### 1. Reduce ç®—å­ä¼˜åŒ–ç ”ç©¶ (`1_cuda_reduce_study/`)

**å­¦ä¹ ç›®æ ‡ï¼š** æ·±å…¥ç†è§£ Reduce ç®—å­çš„ä¼˜åŒ–æŠ€å·§ï¼Œä»åŸºç¡€åˆ°é«˜çº§

**ç‰ˆæœ¬æ¼”è¿›ï¼š**
- `v0_global_memory` - ä½¿ç”¨å…¨å±€å†…å­˜çš„åŸºç¡€ç‰ˆæœ¬
- `v1_shared_memory` - å¼•å…¥å…±äº«å†…å­˜
- `v2_no_divergence_branch` - æ¶ˆé™¤ warp divergence
- `v3_no_bank_conflict` - æ¶ˆé™¤ bank å†²çª
- `v4_add_during_load` - åŠ è½½æ—¶è¿›è¡Œè®¡ç®—ï¼ˆä¸¤ä¸ªæ–¹æ¡ˆï¼‰
- `v5_unroll_last_warp` - å±•å¼€æœ€åä¸€ä¸ª warp
- `v6_completely_unroll` - å®Œå…¨å±•å¼€å¾ªç¯
- `v7_mutli_add` - å¤šå…ƒç´ ç´¯åŠ 
- `v8_shuffle` - ä½¿ç”¨ shuffle æŒ‡ä»¤

**å…³é”®ä¼˜åŒ–æŠ€å·§ï¼š**
- å…±äº«å†…å­˜çš„ä½¿ç”¨å’Œ bank å†²çªé¿å…
- Warp divergence çš„æ¶ˆé™¤
- çº¿ç¨‹åˆ©ç”¨ç‡çš„æå‡
- Shuffle æŒ‡ä»¤çš„ä½¿ç”¨

**ç¼–è¯‘å’Œè¿è¡Œï¼š**
```bash
cd 1_cuda_reduce_study
mkdir build && cd build
cmake ..
make
./my_reduce_v0_global_memory
```

### 2. SGEMM çŸ©é˜µä¹˜æ³•ä¼˜åŒ– (`2_cuda_sgemm_study/`)

**å­¦ä¹ ç›®æ ‡ï¼š** æŒæ¡çŸ©é˜µä¹˜æ³•çš„ç³»ç»Ÿä¼˜åŒ–æ–¹æ³•

**ç‰ˆæœ¬æ¼”è¿›ï¼š**
- `v0_global_memory` - å…¨å±€å†…å­˜ç‰ˆæœ¬
- `v1_shared_memory` - å…±äº«å†…å­˜åˆ†å—
- `v2_shared_memory_sliding_windows` - æ»‘åŠ¨çª—å£ä¼˜åŒ–
- `v3_increase_work_of_per_thread` - å¢åŠ æ¯çº¿ç¨‹å·¥ä½œé‡
- `v4_using_float4` - Float4 å‘é‡åŒ–
- `v5_register_outer_product` - å¯„å­˜å™¨å¤–ç§¯
- `v6_register_outer_product_float4` - å¯„å­˜å™¨å¤–ç§¯ + Float4
- `v7_A_smem_transpose` - A çŸ©é˜µè½¬ç½®ä¼˜åŒ–
- `v8_double_buffer` - åŒç¼“å†²æŠ€æœ¯

**å…³é”®ä¼˜åŒ–æŠ€å·§ï¼š**
- Tiling å’Œå…±äº«å†…å­˜ä½¿ç”¨
- å‘é‡åŒ–å†…å­˜è®¿é—®ï¼ˆfloat4ï¼‰
- å¯„å­˜å™¨çº§ä¼˜åŒ–
- åŒç¼“å†²æµæ°´çº¿

**ç¼–è¯‘å’Œè¿è¡Œï¼š**
```bash
cd 2_cuda_sgemm_study
mkdir build && cd build
cmake ..
make
./my_sgemm_v0_global_memory
```

### 3. Kernel æ€§èƒ½åˆ†ææŒ‡å— (`3_kernel_profiling_guide/`)

**å­¦ä¹ ç›®æ ‡ï¼š** å­¦ä¹ å¦‚ä½•åˆ†æå’Œä¼˜åŒ– Kernel æ€§èƒ½

**ä¸»è¦å†…å®¹ï¼š**
- `my_transpose_v*.cu` - Transpose ç®—å­çš„å¤šä¸ªä¼˜åŒ–ç‰ˆæœ¬
- `roofline_model.cu` - Roofline æ¨¡å‹åˆ†æ
- `combined_access.cu` - åˆå¹¶è®¿é—®æ¨¡å¼

**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**
- `v1_naive` - æœ´ç´ å®ç°
- `v2_float4` - Float4 å‘é‡åŒ–
- `v3_float2` - Float2 å‘é‡åŒ–
- `v4_float2_1x2` - Float2 ä¼˜åŒ–å¸ƒå±€
- `v5_shared_memory` - å…±äº«å†…å­˜ç‰ˆæœ¬
- `v6_no_bank_conflict` - æ¶ˆé™¤ bank å†²çª
- `v7_increase_work_of_per_thread` - å¢åŠ æ¯çº¿ç¨‹å·¥ä½œé‡

**å…³é”®æ¦‚å¿µï¼š**
- Roofline æ¨¡å‹ï¼šç†è§£è®¡ç®—å’Œå†…å­˜å¸¦å®½çš„é™åˆ¶
- å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
- æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

### 4. Tensor Core WMMA (`4_tensor_core_wmma/`)

**å­¦ä¹ ç›®æ ‡ï¼š** å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Tensor Core è¿›è¡Œæ··åˆç²¾åº¦è®¡ç®—

**ç‰ˆæœ¬æ¼”è¿›ï¼š**
- `hgemm_v1_wmma_m16n16k16_naive_kernel` - åŸºç¡€ WMMA ä½¿ç”¨
- `hgemm_v2_wmma_m16n16k16_mma4x2_kernel` - ä¼˜åŒ–ç‰ˆæœ¬
- `hgemm_v3_wmma_m16n16k16_mma4x2_warp2x4_kernel` - å¤š warp ä¼˜åŒ–
- `hgemm_v4_wmma_m16n16k16_mma4x2_warp2x4_dbuf_async_kernel` - å¼‚æ­¥åŒç¼“å†²

**å…³é”®ç‰¹æ€§ï¼š**
- Half precision (FP16) çŸ©é˜µä¹˜æ³•
- WMMA API ä½¿ç”¨
- å¤š warp åä½œ
- å¼‚æ­¥å†…å­˜æ“ä½œ

**é€‚ç”¨æ¶æ„ï¼š** Volta (V100), Turing (T4), Ampere (A100) åŠä»¥ä¸Š

### 5. MMA å’Œ Swizzle (`5_mma_and_swizzle/`)

**å­¦ä¹ ç›®æ ‡ï¼š** å­¦ä¹ é«˜çº§çš„ MMA æŒ‡ä»¤å’Œå†…å­˜è®¿é—®ä¼˜åŒ–

**ç‰ˆæœ¬æ¼”è¿›ï¼š**
- `v1_simple_wmma` - ç®€å• WMMA
- `v2_shared_memory_wmma` - å…±äº«å†…å­˜ WMMA
- `v3_shared_memory_wmma_padding` - Padding ä¼˜åŒ–
- `v4_shared_memory_mma` - MMA æŒ‡ä»¤ä½¿ç”¨
- `v5_shared_memory_mma_swizzle` - Swizzle å†…å­˜è®¿é—®ä¼˜åŒ–

**å…³é”®ä¼˜åŒ–ï¼š**
- MMA (Matrix Multiply-Accumulate) æŒ‡ä»¤
- Shared memory swizzle æ¨¡å¼
- å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–

### 6. CUTLASS å­¦ä¹  (`6_cutlass_study/`)

**å­¦ä¹ ç›®æ ‡ï¼š** å­¦ä¹ ä½¿ç”¨ NVIDIA CUTLASS åº“å®ç°é«˜æ€§èƒ½ GEMM

**å†…å®¹ï¼š**
- `v1_print_half.cu` - Half ç²¾åº¦æ•°æ®ç±»å‹
- `v2_gemm_kernel.cu` - åŸºç¡€ GEMM kernel
- `v3_turing_tensorop_gemm.cu` - Turing æ¶æ„ Tensor Core GEMM

**CUTLASS ç‰¹æ€§ï¼š**
- æ¨¡å—åŒ–çš„ GEMM å®ç°
- æ”¯æŒå¤šç§æ•°æ®ç±»å‹å’Œç²¾åº¦
- é’ˆå¯¹ä¸åŒ GPU æ¶æ„çš„ä¼˜åŒ–

### 7. Flash Attention (`flash_attention/`)

**å­¦ä¹ ç›®æ ‡ï¼š** å­¦ä¹  Flash Attention çš„é«˜æ•ˆå®ç°

**ç‰¹æ€§ï¼š**
- ä½¿ç”¨å…±äº«å†…å­˜é¿å… O(NÂ²) å†…å­˜è®¿é—®
- çº¦ 100 è¡Œ CUDA ä»£ç å®ç°å‰å‘ä¼ æ’­
- ç›¸æ¯”æ ‡å‡†å®ç°æœ‰æ˜¾è‘—åŠ é€Ÿ

**æ€§èƒ½å¯¹æ¯”ï¼š**
- æ ‡å‡† Attention: ~52ms
- Flash Attention: ~4ms (çº¦ 13x åŠ é€Ÿ)

**ç¼–è¯‘å’Œè¿è¡Œï¼š**
```bash
cd flash_attention
mkdir build && cd build
cmake ..
make
python bench.py
```

## ğŸ› ï¸ ç¼–è¯‘è¯´æ˜

### ä½¿ç”¨ CMake

æ‰€æœ‰å­ç›®å½•éƒ½æ”¯æŒ CMake ç¼–è¯‘ï¼š

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
mkdir build && cd build
cmake ..
make

# æˆ–ç¼–è¯‘ç‰¹å®šæ¨¡å—
cd 1_cuda_reduce_study
mkdir build && cd build
cmake ..
make
```

### ç¼–è¯‘é€‰é¡¹

å¯ä»¥åœ¨ `CMakeLists.txt` ä¸­è°ƒæ•´ï¼š
- CUDA æ¶æ„ï¼ˆsm_70, sm_75, sm_80 ç­‰ï¼‰
- ä¼˜åŒ–çº§åˆ«ï¼ˆ-O2, -O3ï¼‰
- è°ƒè¯•é€‰é¡¹

## ğŸ“Š æ€§èƒ½æµ‹è¯•

æ‰€æœ‰ä»£ç éƒ½åŒ…å«æ€§èƒ½æµ‹è¯•ï¼Œå»ºè®®ä½¿ç”¨ Nsight Systems è¿›è¡Œè¯¦ç»†åˆ†æï¼š

```bash
# ä½¿ç”¨ nsys åˆ†æ
nsys profile --trace=cuda,nvtx --output=profile.nsys-rep ./your_kernel

# æŸ¥çœ‹ç»“æœ
nsys-ui profile.nsys-rep
```

## ğŸ“ å­¦ä¹ å»ºè®®

### åˆå­¦è€…

1. ä» `1_cuda_reduce_study` å¼€å§‹ï¼Œç†è§£åŸºç¡€çš„ä¼˜åŒ–æŠ€å·§
2. å­¦ä¹  `3_kernel_profiling_guide`ï¼ŒæŒæ¡æ€§èƒ½åˆ†ææ–¹æ³•
3. å®è·µ `2_cuda_sgemm_study`ï¼Œå­¦ä¹ æ›´å¤æ‚çš„ä¼˜åŒ–

### è¿›é˜¶

1. å­¦ä¹  `4_tensor_core_wmma`ï¼ŒæŒæ¡ Tensor Core ä½¿ç”¨
2. æ·±å…¥ `5_mma_and_swizzle`ï¼Œå­¦ä¹ é«˜çº§ä¼˜åŒ–æŠ€å·§
3. ç ”ç©¶ `6_cutlass_study`ï¼Œäº†è§£å·¥ä¸šçº§å®ç°

### é«˜çº§

1. å®ç° `flash_attention`ï¼Œç†è§£å¤æ‚ç®—æ³•çš„ä¼˜åŒ–
2. ç»“åˆæ€§èƒ½åˆ†æå·¥å…·ï¼Œä¼˜åŒ–è‡ªå·±çš„ä»£ç 
3. é˜…è¯» CUTLASS æºç ï¼Œå­¦ä¹ æœ€ä½³å®è·µ

## ğŸ“– ç›¸å…³èµ„æº

- [NVIDIA CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [CUTLASS Documentation](https://github.com/NVIDIA/cutlass)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)

## ğŸ”— ä¸å…¶ä»–æ¨¡å—çš„å…³ç³»

- **optimize_in_gpu/**: æœ¬ç›®å½•æä¾›äº†æ›´ç³»ç»Ÿã€æ›´æ·±å…¥çš„å­¦ä¹ è·¯å¾„
- **gpu_profile/**: ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·éªŒè¯ä¼˜åŒ–æ•ˆæœ

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **GPU æ¶æ„å…¼å®¹æ€§**ï¼šä¸åŒä»£ç é’ˆå¯¹ä¸åŒ GPU æ¶æ„ï¼Œè¯·æ ¹æ®ä½ çš„ GPU è°ƒæ•´ç¼–è¯‘é€‰é¡¹
2. **æ€§èƒ½æ•°æ®**ï¼šæ‰€æœ‰æ€§èƒ½æ•°æ®ä»…ä¾›å‚è€ƒï¼Œå®é™…æ€§èƒ½å–å†³äºç¡¬ä»¶å’Œé…ç½®
3. **å­¦ä¹ é¡ºåº**ï¼šå»ºè®®æŒ‰ç…§ç¼–å·é¡ºåºå­¦ä¹ ï¼Œæ¯ä¸ªæ¨¡å—éƒ½å»ºç«‹åœ¨å‰ä¸€ä¸ªçš„åŸºç¡€ä¸Š

---

**å¼€å§‹ä½ çš„ CUDA æ·±åº¦å­¦ä¹ ä¹‹æ—…ï¼** ğŸš€

